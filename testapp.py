import os
import time
import tempfile
import shutil
import re
import json
import urllib.parse
from urllib.parse import urljoin, quote
import requests
from bs4 import BeautifulSoup
from io import BytesIO
from pypdf import PdfReader
import streamlit as st
import google.generativeai as genai

# =============================================================================
# âš™ï¸ Î¡Î¥Î˜ÎœÎ™Î£Î•Î™Î£ & API KEY
# =============================================================================

GEMINI_API_KEY = "TO_API_KEY_SOY_EDO"  # Î’Î¬Î»Îµ Ï„Î¿ Î´Î¹ÎºÏŒ ÏƒÎ¿Ï…

st.set_page_config(page_title="AI Legislative Auditor", page_icon="âš–ï¸", layout="wide")

if not GEMINI_API_KEY:
    st.error("âš ï¸ Î›ÎµÎ¯Ï€ÎµÎ¹ Ï„Î¿ GEMINI_API_KEY.")
    st.stop()

genai.configure(api_key=GEMINI_API_KEY)

# =============================================================================
# ğŸ“œ SYSTEM PROMPT
# =============================================================================
SYSTEM_INSTRUCTIONS = """
... Î’Î‘Î›Î• Î•Î”Î© ÎŸÎ›ÎŸ Î¤ÎŸ PROMPT Î ÎŸÎ¥ Î•Î§Î•Î™Î£ ...
"""

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
    "Accept-Language": "el-GR,el;q=0.9,en;q=0.8",
    "Accept-Encoding": "gzip, deflate, br",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1",
    "Referer": "https://www.google.com/"
}

# =============================================================================
# ğŸ› ï¸ API Î’ÎŸÎ¥Î›Î—Î£
# =============================================================================

def get_law_data_from_api(query: str):
    """Î¨Î¬Ï‡Î½ÎµÎ¹ ÏƒÏ„Î¿ API Ï„Î·Ï‚ Î’Î¿Ï…Î»Î®Ï‚ ÎºÎ±Î¹ ÎµÏ€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ Ï„Î¿ Ï€ÏÏÏ„Î¿ Î±Ï€Î¿Ï„Î­Î»ÎµÏƒÎ¼Î± (dict) Î® None."""
    url = "https://www.hellenicparliament.gr/api.ashx"
    params = {"q": "laws", "format": "json"}
    if query.isdigit():
        params["lawnum"] = query
    else:
        params["freetext"] = query

    try:
        r = requests.get(url, params=params, headers=HEADERS, timeout=30)
        if r.status_code != 200:
            st.error(f"API HTTP {r.status_code}: {r.text[:200]}")
            return None

        try:
            data = r.json()
        except json.JSONDecodeError as e:
            st.error(f"API Î´ÎµÎ½ ÎµÏ€Î­ÏƒÏ„ÏÎµÏˆÎµ Î­Î³ÎºÏ…ÏÎ¿ JSON: {e}")
            st.text(r.text[:500])
            return None

        if isinstance(data, dict) and data.get("TotalRecords", 0) > 0:
            items = data.get("Data") or data.get("data") or []
            if items:
                return items[0]

        st.warning("Î¤Î¿ API Î³ÏÏÎ¹ÏƒÎµ Î¬Î´ÎµÎ¹Î¿ Î±Ï€Î¿Ï„Î­Î»ÎµÏƒÎ¼Î±.")
        return None

    except Exception as e:
        st.error(f"API Error: {e}")
        return None

# =============================================================================
# ğŸ§¾ HYBRID PDF (TEXT + OCR)
# =============================================================================

def process_pdf_hybrid(url, file_type):
    """
    ÎšÎ±Ï„ÎµÎ²Î¬Î¶ÎµÎ¹ Ï„Î¿ PDF.
    1. Î ÏÎ¿ÏƒÏ€Î±Î¸ÎµÎ¯ Î½Î± ÎµÎ¾Î¬Î³ÎµÎ¹ ÎºÎµÎ¯Î¼ÎµÎ½Î¿ Î¼Îµ pypdf.
    2. Î‘Î½ Ï„Î¿ ÎºÎµÎ¯Î¼ÎµÎ½Î¿ ÎµÎ¯Î½Î±Î¹ Î»Î¯Î³Î¿ (<500 chars), Ï„Î¿ Î¸ÎµÏ‰ÏÎµÎ¯ ÏƒÎºÎ±Î½Î±ÏÎ¹ÏƒÎ¼Î­Î½Î¿ ÎºÎ±Î¹ Ï„Î¿ Î±Î½ÎµÎ²Î¬Î¶ÎµÎ¹ ÏƒÏ„Î¿ Gemini (OCR).
    """
    if not url:
        return "", None, False

    try:
        if not url.startswith("http"):
            url = "https://www.hellenicparliament.gr" + url

        st.write(f"â¬‡ï¸ Î›Î®ÏˆÎ·: {file_type}...")
        res = requests.get(url, headers=HEADERS, timeout=60)
        res.raise_for_status()

        text_content = ""
        try:
            with BytesIO(res.content) as f:
                reader = PdfReader(f)
                for page in reader.pages:
                    text_content += page.extract_text() or ""
        except Exception:
            pass

        clean_txt = re.sub(r"\s+", " ", text_content).strip()

        if len(clean_txt) > 500:
            return clean_txt, None, False  # Text PDF

        # Fallback ÏƒÎµ OCR (Gemini)
        st.caption(f"âš ï¸ Î¤Î¿ Î±ÏÏ‡ÎµÎ¯Î¿ '{file_type}' Ï†Î±Î¯Î½ÎµÏ„Î±Î¹ ÏƒÎºÎ±Î½Î±ÏÎ¹ÏƒÎ¼Î­Î½Î¿. Î•Î½ÎµÏÎ³Î¿Ï€Î¿Î¯Î·ÏƒÎ· OCR...")
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
            tmp.write(res.content)
            tmp_path = tmp.name

        uploaded_file = genai.upload_file(tmp_path, mime_type="application/pdf")
        return "", uploaded_file, True

    except Exception as e:
        st.warning(f"Î£Ï†Î¬Î»Î¼Î± ÎºÎ±Ï„Î¬ Ï„Î·Î½ ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± Ï„Î¿Ï… {file_type}: {e}")
        return "", None, False

# =============================================================================
# ğŸŒ OPENGOV
# =============================================================================

def find_opengov_smart(law_title):
    """Î¨Î¬Ï‡Î½ÎµÎ¹ ÏƒÏ„Î¿ Google Î³Î¹Î± Î´Î¹Î±Î²Î¿ÏÎ»ÎµÏ…ÏƒÎ· ÏƒÏ„Î¿ Opengov."""
    stopwords = ["ÎšÏÏÏ‰ÏƒÎ·", "Î•Î½ÏƒÏ‰Î¼Î¬Ï„Ï‰ÏƒÎ·", "Î¡Ï…Î¸Î¼Î¯ÏƒÎµÎ¹Ï‚", "Î”Î¹Î±Ï„Î¬Î¾ÎµÎ¹Ï‚", "Ï„Î¿Ï…", "Ï„Î·Î½", "ÎºÎ±Î¹", "Î³Î¹Î±", "Î¼Îµ"]
    words = law_title.split()
    keywords = [w for w in words if len(w) > 3 and w not in stopwords]
    search_query = " ".join(keywords[:6])

    query = f"site:opengov.gr {search_query} Î´Î¹Î±Î²Î¿ÏÎ»ÎµÏ…ÏƒÎ·"
    url = f"https://www.google.com/search?q={urllib.parse.quote(query)}"

    try:
        res = requests.get(url, headers=HEADERS, timeout=10)
        if res.status_code == 200:
            soup = BeautifulSoup(res.text, "html.parser")
            for a in soup.find_all("a", href=True):
                href = a["href"]
                if "opengov.gr" in href and "google" not in href:
                    return href
                if "/url?q=" in href and "opengov.gr" in href:
                    return href.split("/url?q=")[1].split("&")[0]
    except Exception:
        pass

    return None

def scrape_opengov(url):
    """ÎšÎ±Ï„ÎµÎ²Î¬Î¶ÎµÎ¹ ÎºÎ±Î¹ ÎºÎ±Î¸Î±ÏÎ¯Î¶ÎµÎ¹ ÎºÎµÎ¯Î¼ÎµÎ½Î¿ Î±Ï€ÏŒ ÏƒÎµÎ»Î¯Î´Î± Î´Î¹Î±Î²Î¿ÏÎ»ÎµÏ…ÏƒÎ·Ï‚ Opengov."""
    if not url:
        return ""
    try:
        r = requests.get(url, headers=HEADERS, timeout=10)
        soup = BeautifulSoup(r.content, "html.parser")
        for s in soup(["script", "style", "nav", "footer"]):
            s.decompose()
        return re.sub(r"\s+", " ", soup.get_text()).strip()[:20000]
    except Exception:
        return ""

# =============================================================================
